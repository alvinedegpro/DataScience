{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMx+uHMdSxDPgZMNhLJEjHp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Lets start by loading our dataset from local storage\n","\n"],"metadata":{"id":"bgW_LUMtmjKd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM9mVEv4jwyb"},"outputs":[],"source":["from google.colab import files\n","uploaded =files.upload()"]},{"cell_type":"markdown","source":["Check if the file exists"],"metadata":{"id":"wxG2dtyOnLOi"}},{"cell_type":"code","source":["!cat prostate.csv"],"metadata":{"id":"_JVsFpKZnXby"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import relevant libraries"],"metadata":{"id":"lm_s-4oYnvON"}},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"Jnq4Pz8Enul1","executionInfo":{"status":"ok","timestamp":1695599796508,"user_tz":-180,"elapsed":3224,"user":{"displayName":"alvine degpro","userId":"03474550938052465227"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Loading to pandas"],"metadata":{"id":"Acs5amAUngHG"}},{"cell_type":"code","source":["df = pd.read_csv(\"prostate.csv\")\n","df.head()"],"metadata":{"id":"JrPsZ8S5nmWK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Standardize the Variables**\n","\n","Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier than variables that are on a small scale."],"metadata":{"id":"d-DmwArxoEF5"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","\n","scaler.fit(df.drop('Target', axis=1))\n","scaled_features = scaler.transform(df.drop('Target',\n","                                           axis=1))\n","\n","df_feat = pd.DataFrame(scaled_features,\n","                       columns=df.columns[:-1])\n","df_feat.head()"],"metadata":{"id":"HmqvTDX4oL13"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model Development and Evaluation**\n","\n","Now by using the sklearn library implementation of the KNN algorithm we will train a model on that. Also after the training purpose, we will evaluate our model by using the confusion matrix and classification report."],"metadata":{"id":"TsvUS1XEoaVw"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report,\\\n","\tconfusion_matrix\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test,y_train, y_test = train_test_split(scaled_features,df['Target'],test_size=0.30)\n","\n","# Remember that we are trying to come up\n","# with a model to predict whether\n","# someone will Target or not.\n","# We'll start with k = 1.\n","\n","knn = KNeighborsClassifier(n_neighbors=1)\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","\n","# Predictions and Evaluations\n","# Let's evaluate our KNN model !\n","print(confusion_matrix(y_test, pred))\n","print(classification_report(y_test, pred))\n"],"metadata":{"id":"-vILtnCpogIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Elbow Method**\n","\n","Let’s go ahead and use the elbow method to pick a good K Value"],"metadata":{"id":"lcdBtSA3pytK"}},{"cell_type":"code","source":["error_rate = []\n","\n","# Will take some time\n","for i in range(1, 40):\n","\n","    knn = KNeighborsClassifier(n_neighbors=i)\n","    knn.fit(X_train, y_train)\n","    pred_i = knn.predict(X_test)\n","    error_rate.append(np.mean(pred_i != y_test))\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, 40), error_rate, color='blue',\n","         linestyle='dashed', marker='o',\n","         markerfacecolor='red', markersize=10)\n","\n","plt.title('Error Rate vs. K Value')\n","plt.xlabel('K')\n","plt.ylabel('Error Rate')\n","plt.show()"],"metadata":{"id":"8EeF7vShqG8j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we can observe that the error value is oscillating and then it increases to become saturated approximately. So, let’s take the value of K equal to 7 as that value of error is quite redundant"],"metadata":{"id":"jrisYplPqeZL"}},{"cell_type":"code","source":["# FIRST A QUICK COMPARISON TO OUR ORIGINAL K = 1\n","knn = KNeighborsClassifier(n_neighbors = 1)\n","\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","\n","print('WITH K = 1')\n","print('Confusion Matrix')\n","print(confusion_matrix(y_test, pred))\n","print('Classification Report')\n","print(classification_report(y_test, pred))"],"metadata":{"id":"GENp2KeMqi-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["f1 score should be close to 1 for the model to be performing well"],"metadata":{"id":"hO6utIWNq4tK"}},{"cell_type":"code","source":["# NOW WITH K = 7\n","knn = KNeighborsClassifier(n_neighbors = 7)\n","\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","\n","print('WITH K = 7')\n","print('Confusion Matrix')\n","print(confusion_matrix(y_test, pred))\n","print('Classification Report')\n","print(classification_report(y_test, pred))"],"metadata":{"id":"xbUdFaZzq6wT"},"execution_count":null,"outputs":[]}]}